# CITATIONS

## Source 1
- **URL:** https://github.com/tensorflow/tensorflow/blob/56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449/tensorflow/python/keras/optimizer_v2/optimizer_v2.py
- **Title:** TensorFlow Keras OptimizerV2 Base Class (optimizer_v2.py)
- **DOI:** null
- **Type:** Other
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** 56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449

## Source 2
- **URL:** https://github.com/tensorflow/tensorflow/blob/56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449/tensorflow/python/keras/optimizer_v2/adam.py
- **Title:** TensorFlow Keras Adam Optimizer (adam.py)
- **DOI:** null
- **Type:** Other
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** 56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449

## Source 3
- **URL:** https://github.com/tensorflow/tensorflow/blob/56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449/tensorflow/python/keras/optimizer_v2/gradient_descent.py
- **Title:** TensorFlow Keras SGD Optimizer (gradient_descent.py)
- **DOI:** null
- **Type:** Other
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** 56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449

## Source 4
- **URL:** https://github.com/tensorflow/tensorflow/blob/56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449/tensorflow/python/keras/optimizer_v2/adagrad.py
- **Title:** TensorFlow Keras Adagrad Optimizer (adagrad.py)
- **DOI:** null
- **Type:** Other
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** 56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449

## Source 5
- **URL:** https://github.com/tensorflow/tensorflow/blob/56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449/tensorflow/python/keras/optimizer_v2/rmsprop.py
- **Title:** TensorFlow Keras RMSprop Optimizer (rmsprop.py)
- **DOI:** null
- **Type:** Other
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** 56e5f82166a6cf6b4b7bc1202affe0bd8b0c8449

## Source 6
- **URL:** https://www.tensorflow.org/api_docs/python/tf/keras/optimizers
- **Title:** TensorFlow Keras Optimizers API Documentation
- **DOI:** null
- **Type:** Documentation
- **Certainty:** N/A
- **Importance:** Supporting (2)
- **Commit Hash:** N/A

## Source 7
- **URL:** https://arxiv.org/abs/1412.6980
- **Title:** Adam: A Method for Stochastic Optimization (Kingma and Ba, 2014)
- **DOI:** null
- **Type:** Research Paper
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** N/A

## Source 8
- **URL:** https://jmlr.org/papers/v12/duchi11a.html
- **Title:** Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (Duchi et al., 2011)
- **DOI:** null
- **Type:** Research Paper
- **Certainty:** N/A
- **Importance:** Essential (1)
- **Commit Hash:** N/A
